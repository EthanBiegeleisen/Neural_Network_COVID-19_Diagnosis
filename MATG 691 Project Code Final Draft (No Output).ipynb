{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethan Biegeleisen\n",
    "\n",
    "MATG 691 Project\n",
    "\n",
    "December 7, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from Course 4, Week 1, Assignment 2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "from cnnutils import * # Code from original assignment said cnn_utils instead of cnnutils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from Course 4, Week 2, Assignment 1\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 64 x 64 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "import glob\n",
    "\n",
    "Images_Positive64 = []\n",
    "X_Positive64 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive64 = image.load_img(filename, target_size = (64, 64))\n",
    "    x_Positive64 = image.img_to_array(Image_Positive64)\n",
    "    Images_Positive64.append(Image_Positive64)\n",
    "    X_Positive64.append(x_Positive64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive64[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative64 = []\n",
    "X_Negative64 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative64 = image.load_img(filename, target_size = (64, 64))\n",
    "    x_Negative64 = image.img_to_array(Image_Negative64)\n",
    "    Images_Negative64.append(Image_Negative64)\n",
    "    X_Negative64.append(x_Negative64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative64[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive64)):\n",
    "    X_Positive64[i] = X_Positive64[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative64)):\n",
    "    X_Negative64[i] = X_Negative64[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for the image arrays\n",
    "# These labels can be used for any image size\n",
    "\n",
    "Y_Positive = np.ones(len(X_Positive64))\n",
    "Y_Negative = np.zeros(len(X_Negative64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X64 = np.vstack((X_Positive64, X_Negative64))\n",
    "Y64 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train64, X_Test64, Y_Train64, Y_Test64 = train_test_split(X64, Y64, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Convolutional Block Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the convolutional block function for a standard network (no skip connection)\n",
    "\n",
    "def convolutional_block_standard(X, f, filters, s = 2, training = True, initializer = glorot_uniform):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer, \n",
    "                   also called Xavier uniform initializer.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    ##### MAIN PATH #####\n",
    "    \n",
    "    # First component of main path glorot_uniform(seed=0)\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # For the first component, a convolutional layer is applied to the input with filter size F1 x F1, a stride of s, and padding applied to the input \n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    \n",
    "    ## Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding='same', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # For the second component, a convolutional layer is applied to the input with filter size F2 x F2, a stride of 1, and no padding applied to the input\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    \n",
    "    ## Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "\n",
    "    # For the third component, a convolutional layer is applied to the input with filter size F3 x F3, a stride of 1, and padding applied to the input\n",
    "    # This is followed by batch normalization  \n",
    "    \n",
    "    # Final step: Pass the main path through a RELU activation\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # For the final step, the input is passed through a ReLU activation\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the convolutional block function for a residual network (skip connection)\n",
    "\n",
    "def convolutional_block_skip(X, f, filters, s = 2, training = True, initializer = glorot_uniform):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer, \n",
    "                   also called Xavier uniform initializer.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    \n",
    "    # First component of main path glorot_uniform(seed=0)\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # For the first component, a convolutional layer is applied to the input with filter size F1 x F1, a stride of s, and padding applied to the input \n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    \n",
    "    ## Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding='same', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # For the second component, a convolutional layer is applied to the input with filter size F2 x F2, a stride of 1, and no padding applied to the input\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    \n",
    "    ## Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    \n",
    "    # For the third component, a convolutional layer is applied to the input with filter size F3 x F3, a stride of 1, and padding applied to the input\n",
    "    # This is followed by batch normalization    \n",
    "    \n",
    "    ##### SHORTCUT PATH #####\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training=training)\n",
    "\n",
    "    # For shortcut path, a convolutional layer is applied to the initial input with filter size F3 x F3, a stride of s, and padding applied to the input \n",
    "    # This is followed by batch normalization\n",
    "    \n",
    "    # Final step: Add shortcut value to main path (Use this order [X, X_shortcut]), and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # For the final step, the input that went through the third component is added with the input that went through the shorcut path\n",
    "    # This is followed by a ReLU activation\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 64 x 64 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel64(input_shape = (64, 64, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "    \n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "\n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "    \n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model64 = StandardModel64(input_shape = (64, 64, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model64.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model64.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History64 = Standard_Model64.fit(X_Train64, Y_Train64, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History64.history['loss'])\n",
    "plt.title('Standard Network Loss (64 x 64 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History64.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (64 x 64 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss64, Standard_Test_Accuracy64 = Standard_Model64.evaluate(X_Test64, Y_Test64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss64)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 64 x 64 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet64(input_shape = (64, 64, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "    \n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model64 = ResNet64(input_shape = (64, 64, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model64.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model64.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History64 = ResNet_Model64.fit(X_Train64, Y_Train64, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History64.history['loss'])\n",
    "plt.title('Residual Network Loss (64 x 64 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History64.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (64 x 64 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History64.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History64.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (64 x 64 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History64.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History64.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (64 x 64 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss64, ResNet_Test_Accuracy64 = ResNet_Model64.evaluate(X_Test64, Y_Test64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss64)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis64 = [Standard_Test_Loss64, ResNet_Test_Loss64]\n",
    "plt.bar(MethodAxis, LossAxis64, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (64 x 64 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis64 = [Standard_Test_Accuracy64, ResNet_Test_Accuracy64]\n",
    "plt.bar(MethodAxis, AccuracyAxis64, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (64 x 64 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 32 x 32 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "Images_Positive32 = []\n",
    "X_Positive32 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive32 = image.load_img(filename, target_size = (32, 32))\n",
    "    x_Positive32 = image.img_to_array(Image_Positive32)\n",
    "    Images_Positive32.append(Image_Positive32)\n",
    "    X_Positive32.append(x_Positive32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive32[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative32 = []\n",
    "X_Negative32 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative32 = image.load_img(filename, target_size = (32, 32))\n",
    "    x_Negative32 = image.img_to_array(Image_Negative32)\n",
    "    Images_Negative32.append(Image_Negative32)\n",
    "    X_Negative32.append(x_Negative32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative32[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive32)):\n",
    "    X_Positive32[i] = X_Positive32[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative32)):\n",
    "    X_Negative32[i] = X_Negative32[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X32 = np.vstack((X_Positive32, X_Negative32))\n",
    "Y32 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "X_Train32, X_Test32, Y_Train32, Y_Test32 = train_test_split(X32, Y32, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 32 x 32 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel32(input_shape = (32, 32, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "    \n",
    "    \n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model32 = StandardModel32(input_shape = (32, 32, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model32.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History32 = Standard_Model32.fit(X_Train32, Y_Train32, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History32.history['loss'])\n",
    "plt.title('Standard Network Loss (32 x 32 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History32.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (32 x 32 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss32, Standard_Test_Accuracy32 = Standard_Model32.evaluate(X_Test32, Y_Test32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss32)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 32 x 32 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet32(input_shape = (32, 32, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "    \n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "    \n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2    \n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model32 = ResNet32(input_shape = (32, 32, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model32.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model32.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History32 = ResNet_Model32.fit(X_Train32, Y_Train32, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History32.history['loss'])\n",
    "plt.title('Residual Network Loss (32 x 32 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History32.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (32 x 32 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History32.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History32.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (32 x 32 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History32.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History32.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (32 x 32 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss32, ResNet_Test_Accuracy32 = ResNet_Model32.evaluate(X_Test32, Y_Test32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss32)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis32 = [Standard_Test_Loss32, ResNet_Test_Loss32]\n",
    "plt.bar(MethodAxis, LossAxis32, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (32 x 32 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis32 = [Standard_Test_Accuracy32, ResNet_Test_Accuracy32]\n",
    "plt.bar(MethodAxis, AccuracyAxis32, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (32 x 32 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 16 x 16 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "Images_Positive16 = []\n",
    "X_Positive16 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive16 = image.load_img(filename, target_size = (16, 16))\n",
    "    x_Positive16 = image.img_to_array(Image_Positive16)\n",
    "    Images_Positive16.append(Image_Positive16)\n",
    "    X_Positive16.append(x_Positive16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive16[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative16 = []\n",
    "X_Negative16 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative16 = image.load_img(filename, target_size = (16, 16))\n",
    "    x_Negative16 = image.img_to_array(Image_Negative16)\n",
    "    Images_Negative16.append(Image_Negative16)\n",
    "    X_Negative16.append(x_Negative16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative16[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive16)):\n",
    "    X_Positive16[i] = X_Positive16[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative16)):\n",
    "    X_Negative16[i] = X_Negative16[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X16 = np.vstack((X_Positive16, X_Negative16))\n",
    "Y16 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "X_Train16, X_Test16, Y_Train16, Y_Test16 = train_test_split(X16, Y16, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 16 x 16 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel16(input_shape = (16, 16, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model16 = StandardModel16(input_shape = (16, 16, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model16.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History16 = Standard_Model16.fit(X_Train16, Y_Train16, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History16.history['loss'])\n",
    "plt.title('Standard Network Loss (16 x 16 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History16.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (16 x 16 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss16, Standard_Test_Accuracy16 = Standard_Model16.evaluate(X_Test16, Y_Test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss16)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 16 x 16 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet16(input_shape = (16, 16, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Three layers of zero padding are placed around the input\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model16 = ResNet16(input_shape = (16, 16, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model16.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model16.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History16 = ResNet_Model16.fit(X_Train16, Y_Train16, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History16.history['loss'])\n",
    "plt.title('Residual Network Loss (16 x 16 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History16.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (16 x 16 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History16.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History16.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (16 x 16 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History16.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History16.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (16 x 16 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss16, ResNet_Test_Accuracy16 = ResNet_Model16.evaluate(X_Test16, Y_Test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss16)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis16 = [Standard_Test_Loss16, ResNet_Test_Loss16]\n",
    "plt.bar(MethodAxis, LossAxis16, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (16 x 16 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis16 = [Standard_Test_Accuracy16, ResNet_Test_Accuracy16]\n",
    "plt.bar(MethodAxis, AccuracyAxis16, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (16 x 16 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 8 x 8 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "Images_Positive8 = []\n",
    "X_Positive8 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive8 = image.load_img(filename, target_size = (8, 8))\n",
    "    x_Positive8 = image.img_to_array(Image_Positive8)\n",
    "    Images_Positive8.append(Image_Positive8)\n",
    "    X_Positive8.append(x_Positive8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive8[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative8 = []\n",
    "X_Negative8 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative8 = image.load_img(filename, target_size = (8, 8))\n",
    "    x_Negative8 = image.img_to_array(Image_Negative8)\n",
    "    Images_Negative8.append(Image_Negative8)\n",
    "    X_Negative8.append(x_Negative8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative8[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive8)):\n",
    "    X_Positive8[i] = X_Positive8[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative8)):\n",
    "    X_Negative8[i] = X_Negative8[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X8 = np.vstack((X_Positive8, X_Negative8))\n",
    "Y8 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "X_Train8, X_Test8, Y_Train8, Y_Test8 = train_test_split(X8, Y8, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 8 x 8 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel8(input_shape = (8, 8, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((4, 4))(X_input)\n",
    "    \n",
    "    # Four layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "    \n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model8 = StandardModel8(input_shape = (8, 8, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History8 = Standard_Model8.fit(X_Train8, Y_Train8, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History8.history['loss'])\n",
    "plt.title('Standard Network Loss (8 x 8 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History8.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (8 x 8 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss8, Standard_Test_Accuracy8 = Standard_Model8.evaluate(X_Test8, Y_Test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss8)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 8 x 8 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet8(input_shape = (8, 8, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((4, 4))(X_input)\n",
    "    \n",
    "    # Four layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "    \n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model8 = ResNet8(input_shape = (8, 8, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History8 = ResNet_Model8.fit(X_Train8, Y_Train8, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History8.history['loss'])\n",
    "plt.title('Residual Network Loss (8 x 8 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History8.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (8 x 8 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History8.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History8.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (8 x 8 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History8.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History8.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (8 x 8 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss8, ResNet_Test_Accuracy8 = ResNet_Model8.evaluate(X_Test8, Y_Test8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss8)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis8 = [Standard_Test_Loss8, ResNet_Test_Loss8]\n",
    "plt.bar(MethodAxis, LossAxis8, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (8 x 8 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis8 = [Standard_Test_Accuracy8, ResNet_Test_Accuracy8]\n",
    "plt.bar(MethodAxis, AccuracyAxis8, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (8 x 8 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 4 x 4 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "Images_Positive4 = []\n",
    "X_Positive4 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive4 = image.load_img(filename, target_size = (4, 4))\n",
    "    x_Positive4 = image.img_to_array(Image_Positive4)\n",
    "    Images_Positive4.append(Image_Positive4)\n",
    "    X_Positive4.append(x_Positive4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive4[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative4 = []\n",
    "X_Negative4 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative4 = image.load_img(filename, target_size = (4, 4))\n",
    "    x_Negative4 = image.img_to_array(Image_Negative4)\n",
    "    Images_Negative4.append(Image_Negative4)\n",
    "    X_Negative4.append(x_Negative4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative4[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive4)):\n",
    "    X_Positive4[i] = X_Positive4[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative4)):\n",
    "    X_Negative4[i] = X_Negative4[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X4 = np.vstack((X_Positive4, X_Negative4))\n",
    "Y4 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "X_Train4, X_Test4, Y_Train4, Y_Test4 = train_test_split(X4, Y4, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 4 x 4 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel4(input_shape = (4, 4, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((6, 6))(X_input)\n",
    "    \n",
    "    # Six layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "    \n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model4 = StandardModel4(input_shape = (4, 4, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History4 = Standard_Model4.fit(X_Train4, Y_Train4, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History4.history['loss'])\n",
    "plt.title('Standard Network Loss (4 x 4 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History4.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (4 x 4 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss4, Standard_Test_Accuracy4 = Standard_Model4.evaluate(X_Test4, Y_Test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss4)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 4 x 4 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet4(input_shape = (4, 4, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((6, 6))(X_input)\n",
    "    \n",
    "    # Six layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model4 = ResNet4(input_shape = (4, 4, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History4 = ResNet_Model4.fit(X_Train4, Y_Train4, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History4.history['loss'])\n",
    "plt.title('Residual Network Loss (4 x 4 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History4.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (4 x 4 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History4.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History4.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (4 x 4 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History4.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History4.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (4 x 4 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss4, ResNet_Test_Accuracy4 = ResNet_Model4.evaluate(X_Test4, Y_Test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss4)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis4 = [Standard_Test_Loss4, ResNet_Test_Loss4]\n",
    "plt.bar(MethodAxis, LossAxis4, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (4 x 4 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis4 = [Standard_Test_Accuracy4, ResNet_Test_Accuracy4]\n",
    "plt.bar(MethodAxis, AccuracyAxis4, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (4 x 4 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the 2 x 2 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the positive dataset and turning each image into an array\n",
    "\n",
    "Images_Positive2 = []\n",
    "X_Positive2 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/COVID-19 (944 Images)/*.png'):\n",
    "    Image_Positive2 = image.load_img(filename, target_size = (2, 2))\n",
    "    x_Positive2 = image.img_to_array(Image_Positive2)\n",
    "    Images_Positive2.append(Image_Positive2)\n",
    "    X_Positive2.append(x_Positive2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Positive2[851])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing every image from the negative dataset and turning each image into an array\n",
    "\n",
    "Images_Negative2 = []\n",
    "X_Negative2 = []\n",
    "for filename in glob.glob('Documents/Manhattan College/Fall 2021/MATG 691/MATG 691 Project/Aria Data/COVID-19_Lung_CT_Scans/Non-COVID-19/*.png'):\n",
    "    Image_Negative2 = image.load_img(filename, target_size = (2, 2))\n",
    "    x_Negative2 = image.img_to_array(Image_Negative2)\n",
    "    Images_Negative2.append(Image_Negative2)\n",
    "    X_Negative2.append(x_Negative2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(Images_Negative2[821])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the image arrays\n",
    "\n",
    "for i in range(len(X_Positive2)):\n",
    "    X_Positive2[i] = X_Positive2[i]/255.0\n",
    "    \n",
    "for i in range(len(X_Negative2)):\n",
    "    X_Negative2[i] = X_Negative2[i]/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y by stacking the positive and negative image arrays\n",
    "\n",
    "X2 = np.vstack((X_Positive2, X_Negative2))\n",
    "Y2 = np.hstack((Y_Positive, Y_Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and Y into train and test sets\n",
    "\n",
    "X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X2, Y2, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Standard Convolutional Neural Network with 2 x 2 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def StandardModel2(input_shape = (2, 2, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (no skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((7, 7))(X_input)\n",
    "    \n",
    "    # Seven layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_standard(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the standard convolutional block function is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_Model2 = StandardModel2(input_shape = (2, 2, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "Standard_Model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "Standard_Model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "Standard_History2 = Standard_Model2.fit(X_Train2, Y_Train2, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History2.history['loss'])\n",
    "plt.title('Standard Network Loss (2 x 2 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History2.history['accuracy'])\n",
    "plt.title('Standard Network Accuracy (2 x 2 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Standard_Test_Loss2, Standard_Test_Accuracy2 = Standard_Model2.evaluate(X_Test2, Y_Test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', Standard_Test_Loss2)\n",
    "print('Test Accuracy:', Standard_Test_Accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Residual Network with 2 x 2 x 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "def ResNet2(input_shape = (2, 2, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the standard convolutional neural network for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK (skip connection) -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((7, 7))(X_input)\n",
    "    \n",
    "    # Seven layers of zero padding are placed around the input\n",
    "    # The amount of padding is increased to compensate for the smaller input array\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # For Stage 1, a convolutional layer is applied to the input with filter size 64 x 64 and a stride of 2\n",
    "    # This is followed by batch normalization and a ReLU activation\n",
    "    # Lastly, a max pooling layer is applied with filter size 3 x 3 and a stride of 2\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_skip(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "\n",
    "    # For Stage 2, the convolutional block function with a skip connection is applied to the input with filter sizes of 64 x 64, 64 x 64, and 256 x 256\n",
    "    \n",
    "    ## AVGPOOL\n",
    "    X = AveragePooling2D(pool_size = (2, 2))(X)\n",
    "\n",
    "    # An average pooling filter is applied to the input with a filter size 2 x 2\n",
    "    \n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # The final section involves flattening the input into a vector\n",
    "    # The vector is then passed through a dense neural network layer with sigmoid activation\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet_Model2 = ResNet2(input_shape = (2, 2, 3), classes = 1)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_Model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(ResNet_Model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "ResNet_History2 = ResNet_Model2.fit(X_Train2, Y_Train2, epochs = 200, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History2.history['loss'])\n",
    "plt.title('Residual Network Loss (2 x 2 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(ResNet_History2.history['accuracy'])\n",
    "plt.title('Residual Network Accuracy (2 x 2 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss during training\n",
    "\n",
    "# Information about plotting loss: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History2.history['loss'], color = 'c')\n",
    "plt.plot(ResNet_History2.history['loss'], color = 'm')\n",
    "plt.title('Network Training Loss (2 x 2 x 3)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy during training\n",
    "\n",
    "# Information about plotting accuracy: \n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "plt.plot(Standard_History2.history['accuracy'], color = 'c')\n",
    "plt.plot(ResNet_History2.history['accuracy'], color = 'm')\n",
    "plt.title('Network Training Accuracy (2 x 2 x 3)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Standard', 'Residual'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "ResNet_Test_Loss2, ResNet_Test_Accuracy2 = ResNet_Model2.evaluate(X_Test2, Y_Test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Loss:', ResNet_Test_Loss2)\n",
    "print('Test Accuracy:', ResNet_Test_Accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss\n",
    "\n",
    "MethodAxis = ['Standard Network', 'Residual Network']\n",
    "LossAxis2 = [Standard_Test_Loss2, ResNet_Test_Loss2]\n",
    "plt.bar(MethodAxis, LossAxis2, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss with Each Neural Network (2 x 2 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy\n",
    "\n",
    "AccuracyAxis2 = [Standard_Test_Accuracy2, ResNet_Test_Accuracy2]\n",
    "plt.bar(MethodAxis, AccuracyAxis2, width = 0.5)\n",
    "plt.xlabel('Neural Network')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy with Each Neural Network (2 x 2 x 3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Test Loss and Test Accuracy of each Network using each Image Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss of each Standard Convolutional Neural Network\n",
    "\n",
    "SizeAxis = ['64', '32', '16', '8', '4', '2']\n",
    "StandardLossAxisAll = [Standard_Test_Loss64, Standard_Test_Loss32, Standard_Test_Loss16, Standard_Test_Loss8, Standard_Test_Loss4, Standard_Test_Loss2]\n",
    "plt.bar(SizeAxis, StandardLossAxisAll, width = 0.5)\n",
    "plt.xlabel('Image Size')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Standard Network Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy of each Sequential Neural Network\n",
    "\n",
    "StandardAccuracyAxisAll = [Standard_Test_Accuracy64, Standard_Test_Accuracy32, Standard_Test_Accuracy16, Standard_Test_Accuracy8, Standard_Test_Accuracy4, Standard_Test_Accuracy2]\n",
    "plt.bar(SizeAxis, StandardAccuracyAxisAll, width = 0.5)\n",
    "plt.xlabel('Image Size')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Standard Network Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss of each Residual Network\n",
    "\n",
    "ResNetLossAxisAll = [ResNet_Test_Loss64, ResNet_Test_Loss32, ResNet_Test_Loss16, ResNet_Test_Loss8, ResNet_Test_Loss4, ResNet_Test_Loss2]\n",
    "plt.bar(SizeAxis, ResNetLossAxisAll, width = 0.5)\n",
    "plt.xlabel('Image Size')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Residual Network Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy of each Residual Network\n",
    "\n",
    "ResNetAccuracyAxisAll = [ResNet_Test_Accuracy64, ResNet_Test_Accuracy32, ResNet_Test_Accuracy16, ResNet_Test_Accuracy8, ResNet_Test_Accuracy4, ResNet_Test_Accuracy2]\n",
    "plt.bar(SizeAxis, ResNetAccuracyAxisAll, width = 0.5)\n",
    "plt.xlabel('Image Size')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Residual Network Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Loss of every Convolutional Neural Network\n",
    "\n",
    "SizeAxisLabels = ['64', '32', '16', '8', '4', '2']\n",
    "SizeAxis = np.arange(len(SizeAxisLabels))\n",
    "StandardLossAxisAll = [Standard_Test_Loss64, Standard_Test_Loss32, Standard_Test_Loss16, Standard_Test_Loss8, Standard_Test_Loss4, Standard_Test_Loss2]\n",
    "ResNetLossAxisAll = [ResNet_Test_Loss64, ResNet_Test_Loss32, ResNet_Test_Loss16, ResNet_Test_Loss8, ResNet_Test_Loss4, ResNet_Test_Loss2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(SizeAxis - 0.15, StandardLossAxisAll, width = 0.3, color = 'c', label = 'Standard')\n",
    "ax.bar(SizeAxis + 0.15, ResNetLossAxisAll, width = 0.3, color = 'm', label = 'Residual')\n",
    "ax.set_xlabel('Image Size')\n",
    "ax.set_ylabel('Test Loss')\n",
    "ax.set_title('Network Test Loss')\n",
    "ax.set_xticks(SizeAxis)\n",
    "ax.set_xticklabels(SizeAxisLabels)\n",
    "ax.legend(loc = 'upper left')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Test Accuracy of every Convolutional Neural Network\n",
    "\n",
    "SizeAxisLabels = ['64', '32', '16', '8', '4', '2']\n",
    "SizeAxis = np.arange(len(SizeAxisLabels))\n",
    "StandardAccuracyAxisAll = [Standard_Test_Accuracy64, Standard_Test_Accuracy32, Standard_Test_Accuracy16, Standard_Test_Accuracy8, Standard_Test_Accuracy4, Standard_Test_Accuracy2]\n",
    "ResNetAccuracyAxisAll = [ResNet_Test_Accuracy64, ResNet_Test_Accuracy32, ResNet_Test_Accuracy16, ResNet_Test_Accuracy8, ResNet_Test_Accuracy4, ResNet_Test_Accuracy2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(SizeAxis - 0.15, StandardAccuracyAxisAll, width = 0.3, color = 'c', label = 'Standard')\n",
    "ax.bar(SizeAxis + 0.15, ResNetAccuracyAxisAll, width = 0.3, color = 'm', label = 'Residual')\n",
    "ax.set_xlabel('Image Size')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Network Test Accuracy')\n",
    "ax.set_xticks(SizeAxis)\n",
    "ax.set_xticklabels(SizeAxisLabels)\n",
    "ax.legend(loc = 'lower right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
